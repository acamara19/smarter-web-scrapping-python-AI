{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBS_DIR = pathlib.Path().resolve().parent\n",
    "BASE_DIR = NBS_DIR\n",
    "DATASET_DIR = BASE_DIR / \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_client():\n",
    "    return OpenAI(\n",
    "        base_url= 'http://localhost:11434/v1',\n",
    "        api_key= 'ollama', # required, but unused\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_openai_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_and_keywords(content=\"\", client=None, raw=None):\n",
    "    if not isinstance(client, OpenAI):\n",
    "        client = get_openai_client()\n",
    "    system_prompt = \"\".join([\n",
    "        \"You are an expert web scraper and researcher.\",\n",
    "        \"When you get data, you perform expert-level summarization and keyword extraction.\",\n",
    "    ])\n",
    "    prompt_start = \"\".join([\n",
    "        \"Extract a 1-word subject of the text as the top ranked keyword.\",\n",
    "        \"Extract and rank top keywords based on the subject matter of only of the text.\",\n",
    "        \"Rank each keyword based on the keyword's importance to the subject matter of the text.\",\n",
    "        \"Provide a concise summary of the contents of the text\",\n",
    "        \"The summary should not include anything related to the discussion nature of the text.\",\n",
    "        \"The summary should not include anything related to the conversation nature of the text.\",\n",
    "        \"The summary should be a minimum 3 paragraphs.\",\n",
    "        \"Use the following text: \"\n",
    "    ])\n",
    "    prompt_end=\"Using format of \\\"{'summary': <generated-summary>, 'keywords': [{value: 'a', rank: 1}, {value: 'b', rank: 2}, {value: 'c', rank: 3}, {value: 'd', rank: 4}, {value: 'e', rank: 5}]}\\\" return a response with json\"\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"{prompt_start} {content} {prompt_end}\",\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"llama2\",\n",
    "      messages=messages,\n",
    "     response_format={ \"type\" : \"json_object\" }\n",
    "    )\n",
    "    if raw:\n",
    "        return response\n",
    "    try:\n",
    "        return json.loads(response.choices[0].message.content), True\n",
    "    except:\n",
    "        return response.choices[0].message.content, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_summary_and_keywords(\"What is the goal of making code-based projects in Python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_summary_and_keywords(\"What is the value of web scraping data?\", client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_summary_and_keywords(\"What are the top key areas of learning to code and learning to web scrape?\", client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in list(DATASET_DIR.glob(\"**/**/thread.html\"))[:5]:\n",
    "    post_id = path.parent.name\n",
    "    output_path = path.parent / 'pred.json'\n",
    "    output_path_txt = path.parent / 'pred.txt'\n",
    "    detail_path = path.parent / 'detail.json'\n",
    "    post_detail = json.loads(detail_path.read_text())\n",
    "    # print(path)\n",
    "    print(post_detail)\n",
    "    # if output_path.exists():\n",
    "    #     continue\n",
    "    soup = BeautifulSoup(path.read_text(), 'html.parser')\n",
    "    body = soup.find('body')\n",
    "    # parse the scraped data or scrape more\n",
    "    content = body.get_text()\n",
    "    content = content.replace('new | past | comments | ask | show | jobs | submit', '')\n",
    "    content = content.replace('login', '').replace('Hacker News', '')\n",
    "    content = content.replace('| hide | past | favorite |', '')\n",
    "    content = content.replace('| parent', '')\n",
    "    content = content.replace('| next [â€“] ', '')\n",
    "    # print(content)\n",
    "    content = content.strip()\n",
    "    # print(content)\n",
    "    try:\n",
    "        pred_data, is_json = extract_summary_and_keywords(content, client=client)\n",
    "        if is_json:\n",
    "            pred_data = json.dumps(pred_data, indent=4)\n",
    "            output_path.write_text(pred_data)\n",
    "        else:\n",
    "            output_path_txt.write_text(pred_data)\n",
    "    except:\n",
    "        continue\n",
    "    print(post_detail, pred_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
