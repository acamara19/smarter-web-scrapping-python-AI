{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBS_DIR = pathlib.Path().resolve().parent\n",
    "BASE_DIR = NBS_DIR\n",
    "DATASET_DIR = BASE_DIR / \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_client():\n",
    "    return OpenAI(\n",
    "        base_url= 'http://localhost:11434/v1',\n",
    "        api_key= 'ollama', # required, but unused\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_openai_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_and_keywords(content=\"\", client=None, raw=None):\n",
    "    if not isinstance(client, OpenAI):\n",
    "        client = get_openai_client()\n",
    "    system_prompt = \"\".join([\n",
    "        \"You are an expert web scraper and researcher.\",\n",
    "        \"When you get data, you perform expert-level summarization and keyword extraction.\",\n",
    "    ])\n",
    "    prompt_start = \"\".join([\n",
    "        \"Extract a 1-word subject of the text as the top ranked keyword.\",\n",
    "        \"Extract and rank top keywords based on the subject matter of only of the text.\",\n",
    "        \"Rank each keyword based on the keyword's importance to the subject matter of the text.\",\n",
    "        \"Provide a concise summary of the contents of the text\",\n",
    "        \"The summary should not include anything related to the discussion nature of the text.\",\n",
    "        \"The summary should not include anything related to the conversation nature of the text.\",\n",
    "        \"The summary should be a minimum 3 paragraphs.\",\n",
    "        \"Use the following text: \"\n",
    "    ])\n",
    "    prompt_end=\"Using format of \\\"{'summary': <generated-summary>, 'keywords': [{value: 'a', rank: 1}, {value: 'b', rank: 2}, {value: 'c', rank: 3}, {value: 'd', rank: 4}, {value: 'e', rank: 5}]}\\\" return a response with json\"\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"{prompt_start} {content} {prompt_end}\",\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"llama2\",\n",
    "      messages=messages,\n",
    "     response_format={ \"type\" : \"json_object\" }\n",
    "    )\n",
    "    if raw:\n",
    "        return response\n",
    "    try:\n",
    "        return json.loads(response.choices[0].message.content), True\n",
    "    except:\n",
    "        return response.choices[0].message.content, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'summary': 'The goal of making code-based projects in Python is to create software applications that can perform specific tasks or solve problems. This involves writing and organizing code in a logical and efficient manner, using various tools and libraries to streamline the development process. The end result is a functional program that can be used to automate processes, analyze data, or provide other services.',\n",
       "  'keywords': [{'value': 'applications', 'rank': 1},\n",
       "   {'value': 'development', 'rank': 2},\n",
       "   {'value': 'code', 'rank': 3},\n",
       "   {'value': 'organization', 'rank': 4},\n",
       "   {'value': 'efficiency', 'rank': 5}]},\n",
       " True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_summary_and_keywords(\"What is the goal of making code-based projects in Python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'summary': 'Web scraping data can provide valuable insights and information that can be used to inform business decisions. The data collected through web scraping can be analyzed and used to identify patterns, trends, and relationships that may not be apparent through other means. Additionally, web scraping can help organizations save time and resources by automating the process of collecting and analyzing data, rather than relying on manual methods.',\n",
       "  'keywords': [{'value': 'data', 'rank': 1},\n",
       "   {'value': 'insights', 'rank': 2},\n",
       "   {'value': 'information', 'rank': 3},\n",
       "   {'value': 'patterns', 'rank': 4},\n",
       "   {'value': 'trends', 'rank': 5},\n",
       "   {'value': 'competitive advantage', 'rank': 6}]},\n",
       " True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_summary_and_keywords(\"What is the value of web scraping data?\", client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'summary': 'Learning to code and web scraping involve mastering various key areas. For coding, the most important areas include problem-solving, algorithmic thinking, and programming languages. In web scraping, the primary areas are data extraction, data manipulation, and web navigation. Understanding these key areas is crucial for success in both fields.',\n",
       "  'keywords': [{'value': 'coding', 'rank': 1},\n",
       "   {'value': 'web scrape', 'rank': 2},\n",
       "   {'value': 'problem-solving', 'rank': 3},\n",
       "   {'value': 'algorithmic thinking', 'rank': 4},\n",
       "   {'value': 'programming languages', 'rank': 5}]},\n",
       " True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_summary_and_keywords(\"What are the top key areas of learning to code and learning to web scrape?\", client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/acamara/Documents/Dev/CFE_Project/SmartWebScrappingWithPythonAI/dataset/2024-02-13/posts/39356920/thread.html\n",
      "{'id': '39356920', 'text': 'Is something bugging you? (antithesis.com)', 'target_link': 'https://antithesis.com/blog/is_something_bugging_you/', 'score': '1179', 'thread_link': 'https://news.ycombinator.com/item?id=39356920'}\n",
      "{'id': '39356920', 'text': 'Is something bugging you? (antithesis.com)', 'target_link': 'https://antithesis.com/blog/is_something_bugging_you/', 'score': '1179', 'thread_link': 'https://news.ycombinator.com/item?id=39356920'} {\n",
      "    \"summary\": \"Antithesis is a fuzzing platform for software developers.\",\n",
      "    \"keywords\": [\n",
      "        {\n",
      "            \"value\": \"fuzzing\",\n",
      "            \"rank\": 1\n",
      "        },\n",
      "        {\n",
      "            \"value\": \"software development\",\n",
      "            \"rank\": 2\n",
      "        },\n",
      "        {\n",
      "            \"value\": \"testing\",\n",
      "            \"rank\": 3\n",
      "        },\n",
      "        {\n",
      "            \"value\": \"security\",\n",
      "            \"rank\": 4\n",
      "        },\n",
      "        {\n",
      "            \"value\": \"devtools\",\n",
      "            \"rank\": 5\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for path in list(DATASET_DIR.glob(\"**/**/thread.html\"))[:5]:\n",
    "    post_id = path.parent.name\n",
    "    output_path = path.parent / 'pred.json'\n",
    "    output_path_txt = path.parent / 'pred.txt'\n",
    "    detail_path = path.parent / 'detail.json'\n",
    "    post_detail = json.loads(detail_path.read_text())\n",
    "    print(path)\n",
    "    print(post_detail)\n",
    "    # if output_path.exists():\n",
    "    #     continue\n",
    "    soup = BeautifulSoup(path.read_text(), 'html.parser')\n",
    "    body = soup.find('body')\n",
    "    # parse the scraped data or scrape more\n",
    "    content = body.get_text()\n",
    "    content = content.replace('new | past | comments | ask | show | jobs | submit', '')\n",
    "    content = content.replace('login', '').replace('Hacker News', '')\n",
    "    content = content.replace('| hide | past | favorite |', '')\n",
    "    content = content.replace('| parent', '')\n",
    "    content = content.replace('| next [â€“] ', '')\n",
    "    # print(content)\n",
    "    content = content.strip()\n",
    "    # print(content)\n",
    "    try:\n",
    "        pred_data, is_json = extract_summary_and_keywords(content, client=client)\n",
    "        if is_json:\n",
    "            pred_data = json.dumps(pred_data, indent=4)\n",
    "            output_path.write_text(pred_data)\n",
    "        else:\n",
    "            output_path_txt.write_text(pred_data)\n",
    "    except:\n",
    "        continue\n",
    "    print(post_detail, pred_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
